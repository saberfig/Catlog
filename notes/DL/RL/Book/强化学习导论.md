## 第2章 多臂赌博机问题
### 2.1 一个k臂赌博机问题
### 2.2 行动价值方法
### 2.3 10臂赌博机试验
### 2.4 增量实现

<img src="http://latex.codecogs.com/gif.latex?\begin{aligned}
Q_{n+1} &=\frac{1}{n} \sum_{i=1}^{n} R_{i} \\
&=\frac{1}{n}\left(R_{n}+\sum_{i=1}^{n-1} R_{i}\right) \\
&=\frac{1}{n}\left(R_{n}+(n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_{i}\right) \\
&=\frac{1}{n}\left(R_{n}+(n-1) Q_{n}\right) \\
&=\frac{1}{n}\left(R_{n}+n Q_{n}-Q_{n}\right) \\
&=Q_{n}+\frac{1}{n}\left(R_{n}-Q_{n}\right)
\end{aligned}">

新估计 = 旧估计 + 步长[目标 - 旧估计]

 一般使用 <img src="http://latex.codecogs.com/gif.latex?\alpha_t(a)"> 表示步长参数，上公式中 <img src="http://latex.codecogs.com/gif.latex?\alpha_t(a)=\frac{1}{n}">

### 2.5 追踪非平稳问题

上面提到的赌博机问题是典型的奖励概率不随时间变化的问题。生活中还存在不稳定的强化学习问题，在这种情况下，给予最近的奖励比给长期的奖励更重要。常用的方法是使用常量步长参数。

<img src="http://latex.codecogs.com/gif.latex?\begin{aligned}
Q_{n+1} &= Q_n + \alpha(R_n - Q_n) \\
&= \alpha R_n + (1-\alpha)Q_n \\
&= \alpha R_n + (1-\alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\
&= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2 \alpha R_{n-2} + \\
& \qquad \qquad \dots + (1-\alpha)^{n-1}\alpha R_1 + (1-\alpha)^nQ_1 \\
&= (1-\alpha)^nQ_1 + \sum_{i=1}^{n}\alpha(1-\alpha)^{n-i}R_i
\end{aligned}">


步长参数 <img src="http://latex.codecogs.com/gif.latex?\alpha\in(0,1]"> 是常数，这使得 <img src="http://latex.codecogs.com/gif.latex?Q_{n+1}"> 是过去奖励和初始估计 <img src="http://latex.codecogs.com/gif.latex?Q_{1}"> 的加权平均值。其中<img src="http://latex.codecogs.com/gif.latex?(1-\alpha)^n + \sum_{i=1}^{n}\alpha(1-\alpha)^{n-i} = 1">。

### 2.6 乐观的初始值
### 2.7 上限置信区间动作选择(UCB)

<img src="http://latex.codecogs.com/gif.latex?A_t \doteq \mathop{argmax} \limits_{a} \left[Q_t(a) + c \sqrt{\frac{\ln{t}}{N_t(a)}}\right]">

### 2.8 赌博机问题的梯度算法

动作a被选择的概率：

<img src="http://latex.codecogs.com/gif.latex?Pr\{A_t=a\} \doteq \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} \doteq \pi_t(a)">

偏好更新方法：

<img src="http://latex.codecogs.com/gif.latex?\begin{aligned}
H_{t+1}(A_t) &\doteq H_t(A_t) + \alpha(R_t-\overline{R}_t)(1-\pi_t(A_t)), & \\
H_{t+1}(a) &\doteq H_t(a) - \alpha(R_t-\overline{R}_t)\pi_t(a),& a \ne A_t
\end{aligned}">

### 2.9 关联搜索（语境赌博机）

## 第3章 有限马尔可夫决策过程（MDP）
### 3.1 个体环境接口

(公式3-2，四参数函数p)：在状态s执行动作a时，进入状态s'以及回报r的概率
<img src="http://latex.codecogs.com/gif.latex?p(s^\prime,r|s,a) \doteq Pr\{S_t=s^\prime,R_t=r|S_{t-1}=s,A_{t-1}=a\}">

(公式3-3)：所有可能和为1
<img src="http://latex.codecogs.com/gif.latex?\sum_{s^\prime \in \mathcal{S}}\sum_{r \in \mathcal{R}}p(s^\prime,r|s,a)=1, s \in \mathcal{S},a \in \mathcal{A}(s)">

(公式3-4，三参数函数p)：状态转移概率函数
<img src="http://latex.codecogs.com/gif.latex?p(s^\prime|s,a) \doteq Pr\{S_t=s^\prime|S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in\mathcal{R}}p(s^\prime,r|s,a)">

(公式3-5)：状态-动作对的预期奖励

<img src="http://latex.codecogs.com/gif.latex?r(s,a)\doteq\mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in\mathcal{R}}r\sum_{s^\prime\in\mathcal{S}}p(s^\prime,r|s,a)">

(公式3-6)：三元组预期奖励
<img src="http://latex.codecogs.com/gif.latex?r(s,a,s^\prime)\doteq\mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s^\prime\right]=\sum_{r\in\mathcal{R}}r\frac{p(s^\prime,r|s,a)}{p(s^\prime|s,a)}">

### 3.2 目标和奖励
### 3.3 回报和情节

情节任务：当个体-环境交互自然地分解为子序列时，我们称之为情节，例如玩游戏，或任何形式的重复互动。每个情节在称为 终点 状态的特殊状态结束，随后是重置到标准起始状态或从起始状态的标准分布的抽样。 即使你认为情节以不同的方式结束，例如输赢游戏，下一情节的开始也与上一情节的结束无关。 因此，所有这些情节都可以被认为是以相同的终点状态结束，对不同的结果有不同的奖励。具有这种情节的任务被称为 情节任务。

持续任务：在许多情况下，个体-环境交互不会自然地分解为可识别的事件，而是持续不断地进行。 例如，这将是一个自然的方式来制定一个持续的过程控制任务，或具有长寿命的机器人上的应用。我们将这些称之为 持续任务。

(公式3-7)：回报是奖励的总和  
<img src="http://latex.codecogs.com/gif.latex?G_{t} \doteq R_{t+1} +R_{t+2} + R_{t+3} + \dots + R_{T}，">

(公式3-8)：加入衰减因子  
<img src="http://latex.codecogs.com/gif.latex?G_{t} \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}">

(公式3-9)：简化  
<img src="http://latex.codecogs.com/gif.latex?\begin{aligned}
G_{t} &\doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots \\
&= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \dots) \\
&= R_{t+1} + \gamma G_{t+1}
\end{aligned}">

(公式3-10)：奖励为1时的回报  
<img src="http://latex.codecogs.com/gif.latex?G_t = \sum_{k=0}^{\infty}\gamma^k = \frac{1}{1-\gamma}">

### 3.4 情节和持续任务的统一符号

(公式3-11)：统一回报公式  
<img src="http://latex.codecogs.com/gif.latex?G_t\doteq\sum_{k=t+1}^{T} \gamma^{k-t-1} R_k">

### 3.5 策略和价值函数

(公式3-12)：状态s，策略π下的价值函数  
<img src="http://latex.codecogs.com/gif.latex?v_\pi(s) \doteq \mathbb{E}_\pi\left[G_t|S_t=s\right]
= \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|S_t=s\right], s\in \mathbb{S}">

(公式3-13)：策略π，状态s下采取动作a的价值函数  
<img src="http://latex.codecogs.com/gif.latex?q_\pi(s,a) \doteq \mathbb{E}_\pi\left[G_t|S_t=s,A_t=a\right]
= \mathbb{E}_\pi\left[\sum^{\infty}_{k=0}\gamma^kR_{t+k+1}|S_t=s,A_t=a\right]">

(公式3-14)：公式3-12的贝尔曼方程  
<img src="http://latex.codecogs.com/gif.latex?\begin{aligned}
v_\pi(s) &\doteq \mathbb{E}_\pi[G_t|S_t=s] \\
&= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s] \\
&= \sum_a\pi(a|s) \sum_{s^\prime}\sum_r p(s^\prime,r|s,a) \left[r+\gamma\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s^\prime]\right] \\
&= \sum_a\pi(a|s) \sum_{s^\prime,r}p(s^\prime,r|s,a)[r+\gamma v_\pi(s^\prime)], s\in\mathcal{S}
\end{aligned}">

### 3.6 最优策略和最优价值函数
### 3.7 优化和近似

<img src="http://latex.codecogs.com/gif.latex?">


## 第4章 动态规划（DP）

动态规划是指可以用于在给定完整的环境模型是马尔可夫决策过程（MDP）的情况下计算最优策略的算法集合

### 4.1 策略评估
### 4.2 策略提升
### 4.3 策略迭代
### 4.4 价值迭代
### 4.5 异步动态规划
### 4.6 广义策略迭代
### 4.7 动态规划的效率


## 第5章 蒙特卡洛方法（MC）
### 5.1 蒙特卡洛预测
### 5.2 动作价值的蒙特卡洛估计
### 5.3 蒙特卡洛控制
### 5.4 非探索开端的蒙特卡洛控制
### 5.5 通过重要性采样的离策略预测
### 5.6 增量式的实现
### 5.7 离策略蒙特卡洛控制
### 5.8 折扣感知的重要性采样
### 5.9 每决策重要性抽样


## 第6章 时序差分学习（TD）
### 6.1 TD预测
### 6.2 TD预测方法的优势
### 6.3 TD(0)的最优性
### 6.4 Sarsa：在策略TD控制
### 6.5 Q-learning：离策略TD控制
### 6.6 预期的Sarsa
### 6.7 最大化偏差和双学习
### 6.8 游戏，Afterstates和其他特殊情况


## 第7章 n步引导
### 7.1 n步TD预测
### 7.2 n步Sarsa
### 7.3 n步离策略学习
### 7.4 具有控制变量的每个决策方法
### 7.5 无重要性采样的离策略学习：n步树备份算法
### 7.6 统一算法：n步Q(σ)