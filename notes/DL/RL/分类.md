# 基本组成部分

- Agent（智能体）：强化学习训练的主体

- Environment（环境）：

- State（状态）：当前Environment和Agent所处的状态

- Action（行动）：基于当前的State，Agent采取哪些action

- Reward（奖励）：Agent在当前State下，采取了某个特定的action后，环境给予的反馈，可以是正向的也可以是负向的。

# 强化学习训练过程

* 马尔可夫决策过程

<img src="http://latex.codecogs.com/gif.latex?\frac{\partial J}{\partial \theta_k^{(j)}}=\sum_{i:r(i,j)=1}{\big((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\big)x_k^{(i)}}+\lambda \theta_k^{(j)}" />


# 算法分类

## 1. Model-Free和Model-Based（无模型和有模型）

* Model-Free
	* Q-Learning
	* Sarsa
	* Policy Gradients

* Model-Based

## 2. Policy-Based和Value-Based（基于策略【概率】和基于值）

* Policy-Based
	* Policy Grandients

* Value-Based
	* Q-Learning
	* Sarsa

## 3. Monte-Carlo update和Temporal-Difference update（回合更新和单步更新）

* Monte-Carlo update
	* 基础版 Policy Gradients
	* Monte-Carlo Learning

* Temporal-Difference update
	* Q Learning
	* Sarsa
	* 升级版 Policy Gradients

## 4. On-Policy和Off-Policy（在线学习和离线学习）

* On-Policy
	* Sarsa
	* Sarsa()

* Off-Policy
	* Q Learning
	* Deep Q Network

## 5. 强化学习和逆强化学习

DQN DDPG A3C PPO/DPPO

<!-- 分类的主要依据是：只能体是否能完整了解或学习到所在环境的模型

## 免模型学习（Model-Free）

放弃了模型学习，在效率上不太好，但是这种方式更容易实现，也容易在真实场景下调整到很好的状态。

1. 策略优化（Policy Optimization）
	
	### A2C/A3C

	### PPO

2. Q-Learning

	### DQN

	### C51

	



## 有模型学习（Model-Based）

对环境有提前的认知，可以提前考虑规划，但是如果模型跟真实世界不一致，那么在实际使用场景下会表现的不好。 -->
